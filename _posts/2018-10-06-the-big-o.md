---
layout: post
title: "The Big O"
date: 2018-10-06 12:49:47
image: '/assets/img/'
description: 'An important concept that is easily overlooked'
tags:
- BigO
- CompSci
- complexity
categories:
- CompSci
twitter_text:
---

# What is the "Big O"
**Big O**, is a metric used to describe how *complex* an algorithm is. Not understanding is might lead to algorithms or code that is unnecessarily complex.

# The Analogy
You want to send file X to friend A. He lives 500 miles away. If the file was 1MB then sending it using the internet would be the quickest. But what if the file is 100MB? Sure it's slower than transferring 1MB but quicker than driving 500 miles?

Now what about 1TB? or 100TB? Now driving might be quicker. 

# Time Complexity

An algorithm is kinda the same as this 'data transfer'
- Internet Transfer can be described by `O(s)`, where the `s` is the size of the file. So as the size of the files increases the time required to complete the transfer increases.
- Driving could be described as `O(1)`, because driving 500 miles is the same no matter what file size. 1MB and 100TB both take the same time to reach your friend, there for as file size increases time complexity does not increase.

You can also have multiple variables in your run time. For example, the time to paint a fence that `x*y` could be described as `O(xy)`, but if we needed `z` layers of paint then we can say `O(xyz)` as the time required.

We can also use Big O to describe, best case, worst case and expected case. But since best case does not provide much use we disregard it for either worst or expected.  

# Space Complexity
Other than how long it takes to run the algorithm we also care about how much memory the algorithm requires. 

For example if we created an array of size `n`, then we require `O(n)` space, but for a 2-D array of `n*n`, we will need `O(n^2)` space.

Stack space in recursive calls counts too. For e.g the code below takes `O(n)` time and space.
{% highlight java %}
int sum(int n) { /* Ex.1*/
	if (n <= 0) {
		return 0;
	}
	return n + sum(n-1);
}
{% endhighlight %}

Each call adds a level to the stack.
- sum(4)
	- sum(3)
		- sum(2)
			- sum(1)
				- sum(0)

Each of the above calls is added to the call stack and takes up memory.

BUUUUUT! Just because there are `n` calls does not mean it takes `O(n)` space. The below function adds adjacent elements between 0 and n:

{% highlight java %}
int pairSumSequence(int n) { /* Ex.2*/
	int sum = 0:
	for (int i =0; i < n; i++) {
		sum += pairSum(i, i + 1);
	}
	return sum:
}

int pairSum(int a, int b) {
	return a +b;
}
{% endhighlight %}

There is still `O(n)` calls to the **pairSum**, but these calls do not exist simultaneously on the call stack so only `O(1)` space is used.

# Dropping the Constants
It is technically possible for `O(N)` code to run faster than `O(1)` for specific inputs. **Big O** just describes the rate of increase. 

Because of this we can drop the constants. So a code with `O(2N)` can just be called as `O(N)`. Some people think 2N is more precise, but its not.

# Dropping the Non-Dominant Terms
If we have something like

kkmok

