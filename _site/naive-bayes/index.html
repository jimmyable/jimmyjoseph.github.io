<!DOCTYPE html>
<html lang="pt-br">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Naive Bayes</title>
    <meta name="description" content="A quick and simple machine learning model, Naive Bayes">

    <!-- Google Authorship Markup -->
    <link rel="author" href="https://plus.google.com/jimmyjoseph1295?rel=author">

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@jimmyable_">
    <meta name="twitter:title" content="Naive Bayes">
    <meta name="twitter:description" content="A quick and simple machine learning model, Naive Bayes">
    
    <meta property="twitter:image:src" content="http://jimmyjoseph.co.uk/assets/img/">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="http://jimmyjoseph.co.uk/naive-bayes/">
    <meta property="og:title" content="Naive Bayes">
    
    <meta property="og:image" content="http://jimmyjoseph.co.uk/assets/img/">
    
    <meta property="og:description" content="A quick and simple machine learning model, Naive Bayes">
    <meta property="og:site_name" content="Jimmy Joseph">

    <!-- Social: Google+ / Schema.org  -->
    <meta itemprop="name" content="Naive Bayes"/>
    <meta itemprop="description" content="A quick and simple machine learning model, Naive Bayes">
    <meta itemprop="image" content="http://jimmyjoseph.co.uk/assets/img/blog-image.png"/>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/assets/img/icons/favicon.ico" type="image/x-icon" />
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />
    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Jimmy Joseph Blog">
    <meta name="msapplication-TileColor" content="#0562DC">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#0562DC">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://jimmyjoseph.co.uk/naive-bayes/">
    <link rel="alternate" type="application/rss+xml" title="Jimmy Joseph" href="http://jimmyjoseph.co.uk/feed.xml" />
</head>


    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2018-12-24 18:11:47 +0000" class="date">24 Dec 2018</time>
        
        <h1 class="post-title" itemprop="name">Naive Bayes</h1>
        <p itemprop="description" class="subtitle">A quick and simple machine learning model, Naive Bayes</p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="http://jimmyjoseph.co.uk/">Home</a></li>
    
    
    
      <li><a href="http://jimmyjoseph.co.uk/series">Series</a></li>
    
    
    
      <li><a href="http://jimmyjoseph.co.uk/tags">Tags</a></li>
    
    
    
      <li><a href="http://jimmyjoseph.co.uk/about">About Me</a></li>
    
    
    <li><a class="feed" href="http://jimmyjoseph.co.uk/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                <h1 id="naive-bayes">Naive Bayes</h1>

<p>Probability of a single event occuring, and both occuring(joint) can be shown as a venn diagriam below.</p>

<p><img src="/assets/img/IntelAI/NB1.png" alt="Output" class="img-responsive" /></p>

<p>What if we only get <strong>P(Y)</strong> and want to predict the <strong>P(X<code class="highlighter-rouge">|</code>Y)</strong>? This is represented below as conditional probability.</p>

<p><img src="/assets/img/IntelAI/NB2.png" alt="Output" class="img-responsive" /></p>

<p>The joint probability can be caculated then as follows:</p>

<p><img src="/assets/img/IntelAI/NB3.png" alt="Output" class="img-responsive" /></p>

<p>Finally, <strong>Bayes</strong> theorem can be derived from the conditional and join relationship as:</p>

<p><img src="/assets/img/IntelAI/NB4.png" alt="Output" class="img-responsive" /></p>

<p>Furthermore, Bayes theorem can be written as:</p>

<p><img src="/assets/img/IntelAI/NB5.png" alt="Output" class="img-responsive" /></p>

<p>But this is not <strong>Naive Bayes</strong>. While training if we calculate the joint probabilities by expanding all the features it would be difficult. If <em>C</em> is <em>class</em>, and <em>X</em> is <em>features</em> then,</p>

<p><img src="/assets/img/IntelAI/NB6.png" alt="Output" class="img-responsive" /></p>

<h2 id="the-naive-assumption">The Naive Assumption</h2>
<p>If we assume all features are independant of each other then the calculation becomes easier. This is the <strong>Naive</strong> assumption.</p>

<p><img src="/assets/img/IntelAI/NB7.png" alt="Output" class="img-responsive" /></p>

<p>When training Naive Bayes, the class assignment is selected based on <em>maximum a posteriori</em> (MAP) rule:</p>

<p><img src="/assets/img/IntelAI/NB8.png" alt="Output" class="img-responsive" />
This means, the class with the largest value is potetially selected.</p>

<p>We will be using the iris dataset to learn Naive Bayes.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span><span class="p">,</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c">#loading the dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"data/Iris_Data.csv"</span><span class="p">)</span></code></pre></figure>

<p>A quick look at the different datatypes involved.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span><span class="o">.</span><span class="n">dtypes</span></code></pre></figure>

<blockquote>
  <ul>
    <li>sepal_length    float64</li>
    <li>sepal_width     float64</li>
    <li>petal_length    float64</li>
    <li>petal_width     float64</li>
    <li>species          object</li>
    <li><em>dtype: object</em></li>
  </ul>
</blockquote>

<p>Look at the skew values and decide if any transformations need to be applied. Let’s use skew value 0.75 as a threshold.</p>

<p><strong>Data skew primarily refers to a non-uniform distribution in a dataset</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">skew</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">skew</span><span class="p">())</span>
<span class="n">skew</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'skew'</span><span class="p">]</span>
<span class="n">skew</span><span class="p">[</span><span class="s">'too_skewed'</span><span class="p">]</span> <span class="o">=</span> <span class="n">skew</span><span class="p">[</span><span class="s">'skew'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">75</span>
<span class="n">skew</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB10.png" alt="Output" class="img-responsive" />
Fields are not too badly skewed.</p>

<p>Use <code class="highlighter-rouge">sns.pairplot</code> to plot the pairwise correlations and histograms. Use <code class="highlighter-rouge">hue="species"</code> as a keyword argument in order to see the distribution of species.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'species'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB11.png" alt="Output" class="img-responsive" /></p>

<h1 id="gaussian-naive-bayes">Gaussian Naive Bayes</h1>

<p>We will now fit a Naive Bayes classifier to this data to predict “species”.</p>

<p>Different types of NB available:
<img src="/assets/img/IntelAI/NB9.png" alt="Output" class="img-responsive" /></p>

<p>Since iris data is continous, we will be using <strong>GaussianNB</strong></p>

<p>We will be using <code class="highlighter-rouge">cross_val_score</code> to see how well our choice works.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="c">#all data excpet last column</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">species</span>

<span class="n">GNB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">cv_N</span> <span class="o">=</span> <span class="mi">4</span> <span class="c">#Determines the cross-validation splitting strategy. 4-kold fold now</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">GNB</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_N</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span></code></pre></figure>

<blockquote>
  <p>[0.94871795 0.94871795 0.91666667 1.        ]
0.953525641025641</p>
</blockquote>

<h1 id="compaing-naive-bayes">Compaing Naive Bayes’</h1>

<p>Lets compare cross validation scores for all types of Naive Bayes.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span><span class="p">,</span> <span class="n">BernoulliNB</span><span class="p">,</span> <span class="n">MultinomialNB</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">species</span>
<span class="n">nb</span> <span class="o">=</span> <span class="p">{</span><span class="s">'gaussian'</span><span class="p">:</span> <span class="n">GaussianNB</span><span class="p">(),</span>
     <span class="s">'bernoulli'</span><span class="p">:</span> <span class="n">BernoulliNB</span><span class="p">(),</span>
     <span class="s">'mutinominal'</span><span class="p">:</span> <span class="n">MultinomialNB</span><span class="p">()}</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">nb</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>
    <span class="n">scores</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">scores</span></code></pre></figure>

<blockquote>
  <p>{‘bernoulli’: 0.3333333333333333,
 ‘gaussian’: 0.953525641025641,
 ‘mutinominal’: 0.9529914529914529}</p>
</blockquote>

<h1 id="why-is-bernoulli-performing-bad">Why is bernoulli performing bad?</h1>

<p>Looks like BernoulliNB results are very bad, but MultinomialNB is doing a very good job.</p>

<p>BernoulliNB is usually good for binary classification. Since we have three classes it doesn’t work very well.</p>

<h1 id="removing-very-predictive-features">Removing very predictive features</h1>

<p>Looking at the pairplot histograms, we can see that the <code class="highlighter-rouge">petal</code>_features are very predictive so let’s remove and see how it affects the results.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s">'sepal_length'</span><span class="p">,</span> <span class="s">'sepal_width'</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">species</span>

<span class="n">nb</span> <span class="o">=</span> <span class="p">{</span><span class="s">'gaussian'</span><span class="p">:</span> <span class="n">GaussianNB</span><span class="p">(),</span>
     <span class="s">'bernoulli'</span><span class="p">:</span> <span class="n">BernoulliNB</span><span class="p">(),</span>
     <span class="s">'mutinominal'</span><span class="p">:</span> <span class="n">MultinomialNB</span><span class="p">()}</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">nb</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>
    <span class="n">scores</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">scores</span></code></pre></figure>

<blockquote>
  <p>{‘bernoulli’: 0.3333333333333333,
 ‘gaussian’: 0.7879273504273504,
 ‘mutinominal’: 0.6800213675213675}</p>
</blockquote>

<p>The accuracy scores for Gaussian and Multinominal have fallen. Whilst Bernoulli remains unaffected.</p>

<p>The Gaussian model seems to produce better accuracy scores, even with the removal of very predictive features.</p>

<h1 id="limitations-of-naive-assumptions">Limitations of Naive Assumptions</h1>
<p>What happens if we push the naive assumption too much?</p>

<blockquote>
  <p>We will create <strong>0, 1 ,3 ,5 ,10 ,50 ,100</strong> copies of <code class="highlighter-rouge">sepal_length</code> and fit a <code class="highlighter-rouge">GaussianNB</code> for each one.</p>
  <ul>
    <li>Keep track of the save the average <code class="highlighter-rouge">cross_val_score</code>.</li>
    <li>Create a plot of the saved scores over the number of copies.</li>
  </ul>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">species</span>

<span class="n">n_copies</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">create_copies_sepal_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">X_new</span><span class="p">[</span><span class="s">'sepal_length_copy</span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s">'sepal_length'</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X_new</span>


<span class="k">def</span> <span class="nf">get_cross_val_score</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">create_copies_sepal_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_N</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">cv_N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>


<span class="n">avg_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="p">[</span><span class="n">get_cross_val_score</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_copies</span><span class="p">],</span>
    <span class="n">index</span><span class="o">=</span><span class="n">n_copies</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">avg_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s">'number of extra copies of "sepal_length"'</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s">'average accuracy score'</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s">'Decline in Naive Bayes performance'</span><span class="p">);</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB12.png" alt="Output" class="img-responsive" /></p>

<p>Naive Bayes assumes each feature is an independant variable, so we created mulitiple copies of one feature <code class="highlighter-rouge">sepal_length</code>. We see that as the data gets duplicated, the average accuracy scores diminishes.</p>

<h1 id="naive-bayes-on-human-activity-recongnition">Naive Bayes on Human Activity Recongnition</h1>

<p>First we load the data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/Human_Activity_Recognition_Using_Smartphones_Data.csv'</span><span class="p">)</span></code></pre></figure>

<p>Now let’s look at the datatypes involved.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span><span class="o">.</span><span class="n">dtypes</span></code></pre></figure>

<blockquote>
  <ul>
    <li>…</li>
    <li>angle(tBodyAccJerkMean),gravityMean)    float64</li>
    <li>angle(tBodyGyroMean,gravityMean)        float64</li>
    <li>angle(tBodyGyroJerkMean,gravityMean)    float64</li>
    <li>angle(X,gravityMean)                    float64</li>
    <li>angle(Y,gravityMean)                    float64</li>
    <li>angle(Z,gravityMean)                    float64</li>
    <li>Activity                                 object</li>
    <li>Length: 562, dtype: object</li>
  </ul>
</blockquote>

<p>Every feature is a <code class="highlighter-rouge">float64</code>, and the targets are objects.</p>

<p>Now let’s create <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">y</code> from the dataset.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Activity</span></code></pre></figure>

<p>Creating training and test splits.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">y_col</span> <span class="o">=</span> <span class="s">'Activity'</span>

<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">y_col</span><span class="p">]</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span><span class="n">y_data</span><span class="p">,</span>
                                                   <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                   <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></figure>

<p>Fitting <code class="highlighter-rouge">GaussianNB</code> to the training split.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">GNB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">GNB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">GNB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></figure>

<p>Creating a confusion matrix for the predictions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_fscore_support</span> <span class="k">as</span> <span class="n">score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">label_binarize</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">cm</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="c"># Preciision, recall, f-score from the multi-class support function</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">fscore</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">)</span>
    
<span class="c"># The usual way to calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c"># ROC-AUC scores can be calculated by binarizing the data</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">label_binarize</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]),</span>
          <span class="n">label_binarize</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]),</span> 
          <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">)</span>

<span class="c"># Last, the confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">({</span><span class="s">'precision'</span><span class="p">:</span><span class="n">precision</span><span class="p">,</span> <span class="s">'recall'</span><span class="p">:</span><span class="n">recall</span><span class="p">,</span> 
                          <span class="s">'fscore'</span><span class="p">:</span><span class="n">fscore</span><span class="p">,</span> <span class="s">'accuracy'</span><span class="p">:</span><span class="n">accuracy</span><span class="p">,</span>
                          <span class="s">'auc'</span><span class="p">:</span><span class="n">auc</span><span class="p">},</span> 
                         <span class="n">name</span><span class="o">=</span><span class="s">'GaussianNB'</span><span class="p">))</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">metrics</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB13.png" alt="Output" class="img-responsive" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">axList</span> <span class="o">=</span> <span class="n">axList</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">axList</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">);</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB14.png" alt="Output" class="img-responsive" /></p>

<h1 id="discretization">Discretization</h1>
<p>Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals with minimal loss of information.</p>

<p>Now, let’s discretize the Human activity dataset. There are many ways to do this, but we’ll use <code class="highlighter-rouge">pd.DataFrame.rank(pct=True)</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">pct</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
<span class="n">X_discrete</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">pct</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_discrete</span> <span class="o">=</span> <span class="n">X_discrete</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">pct</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Activity</span></code></pre></figure>

<p>Look at the values. They are still not discrete. Modify <code class="highlighter-rouge">X_discrete</code> so that it is indeed discrete. (Hint: try to get the first 2 digits using <code class="highlighter-rouge">.applymap</code>)</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'</span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> <span class="n">x</span>
<span class="n">X_discrete</span> <span class="o">=</span> <span class="n">X_discrete</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">func</span><span class="p">)</span></code></pre></figure>

<p>Split <code class="highlighter-rouge">X_discrete</code> and <code class="highlighter-rouge">y</code> into training and test datasets.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_discrete</span><span class="p">,</span><span class="n">y_data</span><span class="p">,</span>
                                                   <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                   <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></figure>

<p>Fit a MultinomialNB to the training split. Get predictions on the test set.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="n">MNB</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">MNB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">MNB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></figure>

<p>Plot the confusion matrix for predictions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_fscore_support</span> <span class="k">as</span> <span class="n">score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">label_binarize</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">cm</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="c"># Preciision, recall, f-score from the multi-class support function</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">fscore</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">)</span>
    
<span class="c"># The usual way to calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c"># ROC-AUC scores can be calculated by binarizing the data</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">label_binarize</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]),</span>
          <span class="n">label_binarize</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]),</span> 
          <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">)</span>

<span class="c"># Last, the confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">({</span><span class="s">'precision'</span><span class="p">:</span><span class="n">precision</span><span class="p">,</span> <span class="s">'recall'</span><span class="p">:</span><span class="n">recall</span><span class="p">,</span> 
                          <span class="s">'fscore'</span><span class="p">:</span><span class="n">fscore</span><span class="p">,</span> <span class="s">'accuracy'</span><span class="p">:</span><span class="n">accuracy</span><span class="p">,</span>
                          <span class="s">'auc'</span><span class="p">:</span><span class="n">auc</span><span class="p">},</span> 
                         <span class="n">name</span><span class="o">=</span><span class="s">'GaussianNB'</span><span class="p">))</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">metrics</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB15.png" alt="Output" class="img-responsive" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">axList</span> <span class="o">=</span> <span class="n">axList</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">axList</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">);</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/NB16.png" alt="Output" class="img-responsive" /></p>

<p>Credits to <a href="https://software.intel.com/en-us/home">Intel AI Academy</a></p>


            </article>

            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://jimmyjoseph.co.uk/naive-bayes/%20via%20&#64;jimmyable_&hashtags=python,naive bayes,machine learning,bayes,scikit,classification,probability,error metrics,confusion matrix,train test split,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://jimmyjoseph.co.uk/naive-bayes/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://jimmyjoseph.co.uk/naive-bayes/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/assets/img/blog-author.jpg" alt="">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="http://jimmyjoseph.co.uk/about">Jimmy Joseph</a>
        </h3>
        <p class="desc">Author, Blogger, Computer Scientist, Designer, Engineer...</p>
        <a itemprop="email" class="email" href="mailto:jimmyjoseph@outlook.com">jimmyjoseph@outlook.com</a>
    </div>
</section>

            <!-- <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = true;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/naive-bayes/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>
 -->
            <footer>
    <p>Built with <a href="http://jekyllrb.com/" target="_blank"> Jekyll</a> by <span class="love"></span> <a href="https://willianjusten.com.br">Willian Justen</a></p>
</footer>
<script src="/assets/js/main.js"></script>

        </section>
    </body>
</html>
