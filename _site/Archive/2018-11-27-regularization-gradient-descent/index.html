<!DOCTYPE html>
<html lang="pt-br">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Regularization and Gradient Descent</title>
    <meta name="description" content="Regularization or normalization changes the scaling for highly varied data. Stochastic Gradient Descent is also explored.">

    <!-- Google Authorship Markup -->
    <link rel="author" href="https://plus.google.com/jimmyjoseph1295?rel=author">

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@jimmyable_">
    <meta name="twitter:title" content="Regularization and Gradient Descent">
    <meta name="twitter:description" content="Regularization or normalization changes the scaling for highly varied data. Stochastic Gradient Descent is also explored.">
    
    <meta property="twitter:image:src" content="http://localhost:4000/assets/img/">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="http://localhost:4000/Archive/2018-11-27-regularization-gradient-descent/">
    <meta property="og:title" content="Regularization and Gradient Descent">
    
    <meta property="og:image" content="http://localhost:4000/assets/img/">
    
    <meta property="og:description" content="Regularization or normalization changes the scaling for highly varied data. Stochastic Gradient Descent is also explored.">
    <meta property="og:site_name" content="Jimmy Joseph">

    <!-- Social: Google+ / Schema.org  -->
    <meta itemprop="name" content="Regularization and Gradient Descent"/>
    <meta itemprop="description" content="Regularization or normalization changes the scaling for highly varied data. Stochastic Gradient Descent is also explored.">
    <meta itemprop="image" content="http://localhost:4000/assets/img/blog-image.png"/>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/assets/img/icons/favicon.ico" type="image/x-icon" />
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />
    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Jimmy Joseph Blog">
    <meta name="msapplication-TileColor" content="#0562DC">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#0562DC">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/Archive/2018-11-27-regularization-gradient-descent/">
    <link rel="alternate" type="application/rss+xml" title="Jimmy Joseph" href="http://localhost:4000/feed.xml" />
</head>


    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2018-11-27 07:26:47 +0000" class="date">27 Nov 2018</time>
        
        <h1 class="post-title" itemprop="name">Regularization and Gradient Descent</h1>
        <p itemprop="description" class="subtitle">Regularization or normalization changes the scaling for highly varied data. Stochastic Gradient Descent is also explored.</p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="http://localhost:4000/">Home</a></li>
    
    
    
      <li><a href="http://localhost:4000/series">Series</a></li>
    
    
    
      <li><a href="http://localhost:4000/tags">Tags</a></li>
    
    
    
      <li><a href="http://localhost:4000/about">About Me</a></li>
    
    
    <li><a class="feed" href="http://localhost:4000/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                <h1 id="regularization-and-gradient-descent">Regularization and Gradient Descent</h1>

<p>In this worked example we will explore <code class="highlighter-rouge">regression</code>, <code class="highlighter-rouge">polynomial features</code>, and <code class="highlighter-rouge">regularization</code> using very simple sparse data.</p>

<p>First we import the data, which contains and x and y columns of noisy data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span><span class="p">,</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"data/X_Y_Sinusoid_Data.csv"</span><span class="p">)</span></code></pre></figure>

<p>Now we will generate 100 equally spaced <strong>x</strong> data points and over the range of 0 to 1. Using these points we will generate the <strong>y</strong> points of <em>ground truth</em> from the equation $y = sin(2\pi x)$</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_real</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">Y_real</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X_real</span><span class="p">)</span></code></pre></figure>

<p>Let’s see how this looks plotted out.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="p">.</span><span class="n">set_style</span><span class="p">(</span><span class="s">'white'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">set_context</span><span class="p">(</span><span class="s">'talk'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s">'inferno'</span><span class="p">)</span>

<span class="c1">#plotting the noisy input data
</span><span class="n">ax</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'x'</span><span class="p">)[</span><span class="s">'y'</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s">''</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">)</span>
<span class="c1">#plotting the ground truth
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_real</span><span class="p">,</span> <span class="n">Y_real</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ground truth'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'y'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD1.png" alt="Output" class="img-responsive" /></p>

<h1 id="polynomial-features">Polynomial Features</h1>

<p>First we use <code class="highlighter-rouge">PolynomialFeatures</code> to create 20th order polynomial features. 20 because we have 20 data points currently.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">degree</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span></code></pre></figure>

<p>Now we will fit the data using <code class="highlighter-rouge">LinearRegression</code></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s">'x'</span><span class="p">]]</span>
<span class="n">Y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'y'</span><span class="p">]</span>

<span class="c1">#fitting the model
</span><span class="n">X_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y_data</span><span class="p">)</span>
<span class="n">Y_pred_lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_poly</span><span class="p">)</span></code></pre></figure>

<p>Let’s plot the results.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_real</span><span class="p">,</span> <span class="n">Y_real</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'ground truth'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_pred_lr</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'polynomial prediction'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'x data'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'y data'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD2.png" alt="Output" class="img-responsive" /></p>

<h1 id="regularization">Regularization</h1>
<p>Using the polynomial features data we can perform the regression. <code class="highlighter-rouge">RidgeRegression</code>(L2) using $\alpha$=0.001, and <code class="highlighter-rouge">LassoRegression</code>(L1) using $\alpha$=0.0001</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span>

<span class="c1">#ridge regression model
</span><span class="n">rr</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">rr</span> <span class="o">=</span> <span class="n">rr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y_data</span><span class="p">)</span>
<span class="n">Y_pred_rr</span> <span class="o">=</span> <span class="n">rr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_poly</span><span class="p">)</span>

<span class="c1">#lasso regression model
</span><span class="n">lassor</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">lassor</span> <span class="o">=</span> <span class="n">lassor</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y_data</span><span class="p">)</span>
<span class="n">Y_pred_lassor</span> <span class="o">=</span> <span class="n">lassor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_poly</span><span class="p">)</span></code></pre></figure>

<p>Plotting the predicted results again.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_real</span><span class="p">,</span> <span class="n">Y_real</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ground truth'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_pred_lr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'linear regression'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_pred_rr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ridge regression'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">Y_pred_lassor</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'lasso regression'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s">'y'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD3.png" alt="Output" class="img-responsive" /></p>

<p>The lasso and ridge regression seems to be much better fit. Lets see how the magnitude of their coefficients compare to the liner regression.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">coeffs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">coeffs</span><span class="p">[</span><span class="s">'linear_regression'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">coeffs</span><span class="p">[</span><span class="s">'ridge_regression'</span><span class="p">]</span> <span class="o">=</span> <span class="n">rr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">coeffs</span><span class="p">[</span><span class="s">'lasso_regression'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lassor</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">coeffs</span><span class="p">.</span><span class="n">applymap</span><span class="p">(</span><span class="nb">abs</span><span class="p">)</span> <span class="c1">#makes all values absolutes
</span>
<span class="n">coeffs</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD4.png" alt="Output" class="img-responsive" /></p>

<p>PLotting the maginitude of coefficients. The Linear Regression will be plotted on a different y-axis since its magnitude is very large.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()</span>

<span class="c1">#setting up both y-axes
</span><span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">twinx</span><span class="p">()</span>

<span class="c1">#linear regression data
</span><span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s">'linear regression'</span><span class="p">)</span>

<span class="c1">#ridge and lasso
</span><span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s">'ridge regression'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lassor</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">'lasso regression'</span><span class="p">)</span>

<span class="c1">#axes scales
</span><span class="n">ax1</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">2e14</span><span class="p">,</span> <span class="mf">2e14</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>

<span class="c1">#combine the legends
</span><span class="n">h1</span><span class="p">,</span> <span class="n">l1</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">h2</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">ax2</span><span class="p">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">h1</span><span class="o">+</span><span class="n">h2</span><span class="p">,</span> <span class="n">l1</span><span class="o">+</span><span class="n">l2</span><span class="p">)</span>

<span class="c1">#plot axes labels
</span><span class="n">ax1</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'coefficients'</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s">'linear regression'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s">'ridge and lasso regression'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD5.png" alt="Output" class="img-responsive" /></p>

<p>The large magnitudes of the data caused overfitting. Regularization penalised the bigger weights by modifying the cost function. More about this <a href="https://www.commonlounge.com/discussion/e3561f62d9c848c4b936f8d5abbdd1b3">here</a>.</p>

<h1 id="test-train-split-and-skewing">Test-Train split and Skewing</h1>
<p>This example uses the Ames Housing prices data.</p>

<p>Firstly, importing the data. Then removing all null values, and one-hot encoding categoricals.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'Intel-ML101-Class4/data/Ames_Housing_Sales.csv'</span><span class="p">)</span>

<span class="c1">#get a pd.series of all string categs
</span><span class="n">one_hot_encode_cols</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">dtypes</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nb">object</span><span class="p">]</span> <span class="c1">#filtering string categs
</span><span class="n">one_hot_encode_cols</span> <span class="o">=</span> <span class="n">one_hot_encode_cols</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1">#list of categ fields
</span>
<span class="c1">#encoding these columns as categories means o-h-c will work even when the data is split
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">one_hot_encode_cols</span><span class="p">:</span>
    <span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>

<span class="c1">#one hot encoding
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">one_hot_encode_cols</span><span class="p">)</span></code></pre></figure>

<p>Splitting the data into test and train data sets.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></figure>

<p>Some columns have skewed features, to which we will apply log transformations. The label <code class="highlighter-rouge">SalePrice</code> also contains some. but we will be ignoring it for now.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#list of float columns to check for skewing
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nb">float</span>
<span class="n">float_cols</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="n">skew_limit</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">skew_vals</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">float_cols</span><span class="p">].</span><span class="n">skew</span><span class="p">()</span>

<span class="n">skew_cols</span> <span class="o">=</span> <span class="p">(</span><span class="n">skew_vals</span>
            <span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="p">.</span><span class="n">to_frame</span><span class="p">()</span>
            <span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'Skew'</span><span class="p">})</span>
            <span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">'abs(Skew) &gt; {0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">skew_limit</span><span class="p">)))</span>

<span class="n">skew_cols</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD6.png" alt="Output" class="img-responsive" /></p>

<p>Transform all the columns with skew &gt; 0.75, apart from <code class="highlighter-rouge">SalePrice</code>.</p>

<p>Let’s visualise at what happens to one of these features after we apply <code class="highlighter-rouge">np.log1p</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">feature</span> <span class="o">=</span> <span class="s">"BsmtFinSF1"</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax_before</span><span class="p">,</span> <span class="n">ax_after</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">train</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">hist</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_before</span><span class="p">)</span>
<span class="n">train</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">).</span><span class="n">hist</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_after</span><span class="p">)</span>

<span class="n">ax_before</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'before np.log1p'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'frequency'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">'value'</span><span class="p">)</span>
<span class="n">ax_after</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'after np.log1p'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'frequency'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">'value'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">'Field "{}"'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">));</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD7.png" alt="Output" class="img-responsive" /></p>

<p>Applying to all the features.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">skew_cols</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">==</span> <span class="s">"SalePrice"</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">train</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
    <span class="n">test</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">)</span></code></pre></figure>

<p>Separating features from predictor.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="s">'SalePrice'</span><span class="p">]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span></code></pre></figure>

<h1 id="linear-regression-and-rmse">Linear Regression and RMSE</h1>

<p>We will create a function <code class="highlighter-rouge">rmse</code> which calculates the <strong>root mean squared error</strong> using the sklearn’s <code class="highlighter-rouge">mean_squared_error</code> method.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="k">def</span> <span class="nf">rmse</span> <span class="p">(</span><span class="n">ytrue</span><span class="p">,</span> <span class="n">ypredicted</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ytrue</span><span class="p">,</span> <span class="n">ypredicted</span><span class="p">))</span></code></pre></figure>

<p>Fitting a basic linear regression model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">linearRegression</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></figure>

<p>Printing the root mean-squared error for this model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">linearRegression_rmse</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">linearRegression</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">linearRegression_rmse</span><span class="p">)</span></code></pre></figure>

<blockquote>
  <p>306369.68342316244</p>
</blockquote>

<p>Plotting the predicted and actual sale price based on this model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">axes</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">linearRegression</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mf">4.0</span><span class="p">)</span>

<span class="n">lim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>

<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'Actual Price'</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="s">'Predicted Price'</span><span class="p">,</span>
      <span class="n">ylim</span><span class="o">=</span><span class="n">lim</span><span class="p">,</span>
      <span class="n">xlim</span><span class="o">=</span><span class="n">lim</span><span class="p">,</span>
      <span class="n">title</span><span class="o">=</span><span class="s">'Linear Regression Results'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD8.png" alt="Output" class="img-responsive" /></p>

<h1 id="rigde-regression">Rigde Regression</h1>
<p>Rigde Regression uses L2 normalization to reduce the magnitude of the coefficients. This can be helpful when there is high variance in the data. The regularization functions in sklearn has cross-validation built in.</p>

<p>We are going to fit a (non-cross validated) Ridge model to a range of $\alpha$ values and plot the RMSE using cross validated error function created above.</p>

<p>Alpha values:
    \([0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\)</p>

<p>Now for the <code class="highlighter-rouge">RidgeCV</code> method, it’s not possible to get the alpha values for the models that weren’t selected. The resulting error values and $\alpha$ values are very similair to what is obtained above.</p>

<p>Finally we can compare the error values of prior and the Ridge models.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>

<span class="n">ridgeCV</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span>
                 <span class="n">cv</span><span class="o">=</span><span class="mi">4</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ridgeCV_rmse</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ridgeCV</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">ridgeCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">ridgeCV_rmse</span><span class="p">)</span></code></pre></figure>

<blockquote>
  <p>15.0 32169.176205672433</p>
</blockquote>

<h1 id="lasso-regression">Lasso Regression</h1>

<p>Just like <code class="highlighter-rouge">RidgeCV</code>, <code class="highlighter-rouge">LassoCV</code> is a function but uses L1 regularization. L1 regularization will selectively shrink some coefficients, which is effectively performing feature elimination.</p>

<p><code class="highlighter-rouge">LassoCV</code> does not have allow a scoring function but the <code class="highlighter-rouge">rmse</code> function created above can be used for model evaluation.</p>

<p><em>There is also <code class="highlighter-rouge">ElasticNetCV</code> which uses a combination of both L1 and L2 normalisation/regularisation</em></p>

<p>Using alphas:
\([1e-5, 5e-5, 0.0001, 0.0005]\)</p>

<p>Fit a Lasso model using cross validation and determine the optimum value for $\alpha$, and the RMSE using the function above. The magnitude of alphas can be different to the Ridge model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>

<span class="n">alphas2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">5e-5</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">])</span>

<span class="n">lassoCV</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas2</span><span class="p">,</span> 
                 <span class="n">max_iter</span><span class="o">=</span><span class="mf">5e4</span><span class="p">,</span>
                 <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">lassoCV_rmse</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lassoCV</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">lassoCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">lassoCV_rmse</span><span class="p">)</span> <span class="c1">#Lasso is slower</span></code></pre></figure>

<blockquote>
  <p>0.0005 39257.393991448225</p>
</blockquote>

<p>We can determine how many of these features remain non-zero.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">'Of {} coefficients, {} are non-zero with Lasso.'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lassoCV</span><span class="p">.</span><span class="n">coef_</span><span class="p">),</span>
                                                              <span class="nb">len</span><span class="p">(</span><span class="n">lassoCV</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])))</span></code></pre></figure>

<blockquote>
  <p>Of 294 coefficients, 274 are non-zero with Lasso.</p>
</blockquote>

<h1 id="elasticnet">ElasticNet</h1>

<p>Using the same alphas as Lasso, we will now try an <code class="highlighter-rouge">ElasticNetCV</code>, with L1 ratios between 0.1 and 0.9</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>

<span class="n">l1_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="n">elasticNetCV</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas2</span><span class="p">,</span>
                           <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratios</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="o">=</span><span class="mf">1e4</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">elasticNetCV_rmse</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">elasticNetCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">l1_ratio_</span><span class="p">,</span> <span class="n">elasticNetCV_rmse</span><span class="p">)</span></code></pre></figure>

<blockquote>
  <p>0.0005 0.1 35001.23429607454</p>
</blockquote>

<p>Its easiest to compare the RMSE calculations for all models in a table.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rmse_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">linearRegression_rmse</span><span class="p">,</span> <span class="n">ridgeCV_rmse</span><span class="p">,</span> <span class="n">lassoCV_rmse</span><span class="p">,</span> <span class="n">elasticNetCV_rmse</span><span class="p">]</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Linear'</span><span class="p">,</span> <span class="s">'Ridge'</span><span class="p">,</span> <span class="s">'Lasso'</span><span class="p">,</span><span class="s">'ElasticNet'</span><span class="p">]</span>

<span class="n">rmse_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">rmse_vals</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">labels</span><span class="p">).</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">rmse_df</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'RMSE'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rmse_df</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD9.png" alt="Output" class="img-responsive" /></p>

<p>Plotting the actual and predicted housing prices as before.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">axes</span><span class="p">()</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Ridge'</span><span class="p">,</span> <span class="s">'Lasso'</span><span class="p">,</span> <span class="s">'ElasticNet'</span><span class="p">]</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">ridgeCV</span><span class="p">,</span> <span class="n">lassoCV</span><span class="p">,</span> <span class="n">elasticNetCV</span><span class="p">]</span>

<span class="k">for</span> <span class="n">mod</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
           <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">lab</span><span class="p">)</span>
    
<span class="n">leg</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">leg</span><span class="p">.</span><span class="n">get_frame</span><span class="p">().</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">leg</span><span class="p">.</span><span class="n">get_frame</span><span class="p">().</span><span class="n">set_linewidth</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'Actual Price'</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="s">'Predicted Price'</span><span class="p">,</span>
      <span class="n">title</span><span class="o">=</span><span class="s">'Linear Regression Results'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD10.png" alt="Output" class="img-responsive" /></p>

<h1 id="stochastic-gradient-descent">Stochastic Gradient Descent</h1>

<p>Linear Models in general are sensitive to scaling. SGD is <strong>very senstive</strong> to scaling</p>

<p>And a high value of learning rate can cause the algorithmn to diverge, while too low of a value may take too long to converge.</p>

<p>Fitting a stochastic gradient descent model without a regularization penalty(the relavant parameter is <code class="highlighter-rouge">penalty</code>)</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Import SGDRegressor and prepare the parameters
</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>

<span class="n">model_parameters_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'Linear'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'none'</span><span class="p">},</span>
    <span class="s">'Lasso'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l2'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">lassoCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">},</span>
    <span class="s">'Ridge'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l1'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">ridgeCV_rmse</span><span class="p">},</span>
    <span class="s">'ElasticNet'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'elasticnet'</span><span class="p">,</span> 
                   <span class="s">'alpha'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span>
                   <span class="s">'l1_ratio'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">l1_ratio_</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">new_rmses</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">modellabel</span><span class="p">,</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">model_parameters_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># following notation passes the dict items as arguments
</span>    <span class="n">SGD</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
    <span class="n">SGD</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">new_rmses</span><span class="p">[</span><span class="n">modellabel</span><span class="p">]</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">SGD</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="n">rmse_df</span><span class="p">[</span><span class="s">'RMSE-SGD'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">new_rmses</span><span class="p">)</span>
<span class="n">rmse_df</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD11.png" alt="Output" class="img-responsive" /></p>

<p>We can see that the error values are vert high. This means the algorithm is divering, and can be due to high scaling/learning rate.</p>

<p>What happens if we adjust the learning rate?</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Import SGDRegressor and prepare the parameters
</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>

<span class="n">model_parameters_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'Linear'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'none'</span><span class="p">},</span>
    <span class="s">'Lasso'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l2'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">lassoCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">},</span>
    <span class="s">'Ridge'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l1'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">ridgeCV_rmse</span><span class="p">},</span>
    <span class="s">'ElasticNet'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'elasticnet'</span><span class="p">,</span> 
                   <span class="s">'alpha'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span>
                   <span class="s">'l1_ratio'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">l1_ratio_</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">new_rmses</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">modellabel</span><span class="p">,</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">model_parameters_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># following notation passes the dict items as arguments
</span>    <span class="n">SGD</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">eta0</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
    <span class="n">SGD</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">new_rmses</span><span class="p">[</span><span class="n">modellabel</span><span class="p">]</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">SGD</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="n">rmse_df</span><span class="p">[</span><span class="s">'RMSE-SGD-learningrate'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">new_rmses</span><span class="p">)</span>
<span class="n">rmse_df</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD12.png" alt="Output" class="img-responsive" /></p>

<p>The error values are much lower. Now let’s try scaling the training data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Import SGDRegressor and prepare the parameters
</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>

<span class="n">model_parameters_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'Linear'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'none'</span><span class="p">},</span>
    <span class="s">'Lasso'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l2'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">lassoCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">},</span>
    <span class="s">'Ridge'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l1'</span><span class="p">,</span>
           <span class="s">'alpha'</span><span class="p">:</span> <span class="n">ridgeCV_rmse</span><span class="p">},</span>
    <span class="s">'ElasticNet'</span><span class="p">:</span> <span class="p">{</span><span class="s">'penalty'</span><span class="p">:</span> <span class="s">'elasticnet'</span><span class="p">,</span> 
                   <span class="s">'alpha'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">alpha_</span><span class="p">,</span>
                   <span class="s">'l1_ratio'</span><span class="p">:</span> <span class="n">elasticNetCV</span><span class="p">.</span><span class="n">l1_ratio_</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">new_rmses</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">modellabel</span><span class="p">,</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">model_parameters_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># following notation passes the dict items as arguments
</span>    <span class="n">SGD</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
    <span class="n">SGD</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">new_rmses</span><span class="p">[</span><span class="n">modellabel</span><span class="p">]</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">SGD</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">))</span>

<span class="n">rmse_df</span><span class="p">[</span><span class="s">'RMSE-SGD-learningrate-Scaled'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">new_rmses</span><span class="p">)</span>
<span class="n">rmse_df</span></code></pre></figure>

<p><img src="/assets/img/IntelAI/SGD13.png" alt="Output" class="img-responsive" /></p>

<p>Credits to <a href="https://software.intel.com/en-us/home">Intel AI Academy</a></p>


            </article>

            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://localhost:4000/Archive/2018-11-27-regularization-gradient-descent/%20via%20&#64;jimmyable_&hashtags=python,regularization,machine learning,numpy,scikit,stochastic gradient descent,ridge regression,scaling,lasso regression,elastic net,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/Archive/2018-11-27-regularization-gradient-descent/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://localhost:4000/Archive/2018-11-27-regularization-gradient-descent/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/assets/img/blog-author.jpg" alt="">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="http://jimmyjoseph.co.uk/about">Jimmy Joseph</a>
        </h3>
        <p class="desc">Author, Blogger, Computer Scientist, Designer, Engineer...</p>
        <a itemprop="email" class="email" href="mailto:jimmyjoseph@outlook.com">jimmyjoseph@outlook.com</a>
    </div>
</section>

            <!-- <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = true;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/Archive/2018-11-27-regularization-gradient-descent/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>
 -->
            <footer>
    <p>Built with <a href="http://jekyllrb.com/" target="_blank"> Jekyll</a> by <span class="love"></span> <a href="https://willianjusten.com.br">Willian Justen</a></p>
</footer>
<script src="/assets/js/main.js"></script>

        </section>
    </body>
</html>
